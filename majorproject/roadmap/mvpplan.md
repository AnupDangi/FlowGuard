# FlowGuard MVP Progress Tracker

## âœ… Phase 1: Real-Time Event Streaming Infrastructure (COMPLETED)

### âœ… Stage 1: Events Gateway + Kafka Cluster (COMPLETED)

**Status:** Production-ready, pushed to GitHub

**Implemented:**

- âœ… FastAPI Events Gateway on port 8000
- âœ… Kafka cluster with 3 brokers (localhost:19092, 19093, 19094)
- âœ… Topics created: `raw.orders.v1` (3 partitions, RF=2), `raw.clicks.v1` (6 partitions, RF=2)
- âœ… Zookeeper on port 2181
- âœ… Kafka UI on port 8080 for visual monitoring
- âœ… Event schemas with Pydantic validation (OrderEvent, ClickEvent)
- âœ… Server-side UUID generation (production-ready, no collision risk)
- âœ… PostgreSQL integration for orders table (DB-first architecture)
- âœ… Orders stored in database before Kafka emission (source of truth)

**Architecture Flow:**

```
Client â†’ Events Gateway â†’ PostgreSQL (INSERT) â†’ Kafka â†’ Real-time Processing
```

---

### âœ… Stage 2: Reference Data Service + Web UI (COMPLETED)

**Status:** Production-ready, pushed to GitHub

**Implemented:**

- âœ… PostgreSQL database (port 5432) with food_catalog
- âœ… Food Catalog Service (FastAPI, port 8001) - reference data API
- âœ… 25 food items with detailed descriptions loaded from JSON
- âœ… Foods table with: food_id, name, category, price, description, image_url, is_available
- âœ… Orders table with: order_id (UUID), user_id, item_id, item_name, price, status, created_at
- âœ… Next.js 16 web app (port 3000) with TypeScript, Tailwind CSS 4
- âœ… Food catalog UI with category filtering
- âœ… Food detail page (/food/[foodId]) with full descriptions
- âœ… Order confirmation page (/order/[orderId]) with server-generated order_id
- âœ… Event tracking: impressions (hover), clicks (card), orders (button)
- âœ… Session-based user tracking with localStorage
- âœ… Client receives server-generated UUIDs (no client-side ID generation)

**API Endpoints:**

- GET /api/foods - List all food items with filtering
- GET /api/foods/{id} - Get single food item
- GET /api/foods/categories/list - List all categories
- POST /api/v1/orders/ - Submit order (returns server-generated order_id)
- POST /api/v1/clicks/ - Submit click/impression event

---

### âœ… Monitoring & Observability (COMPLETED)

**Status:** Production-ready

**Implemented:**

- âœ… Real-time event monitor (scripts/monitor_events.py) - colored Kafka consumer
- âœ… Service log aggregator (scripts/monitor_services.sh) - tail multiple logs
- âœ… Unified startup script (scripts/start_all.sh)
- âœ… Health check endpoints for all services
- âœ… Docker Compose orchestration with health checks
- âœ… Comprehensive monitoring guide (MONITORING_GUIDE.md)

---

## ðŸš§ Phase 2: Data Lake + Batch Processing (NEXT)

### Stage 3: Snowflake Bronze Layer (TODO)

**Goal:** Consume Kafka events and store in data warehouse

**To Implement:**

- [ ] Kafka consumer to read from raw.orders.v1 and raw.clicks.v1
- [ ] Snowflake BRONZE schema setup
- [ ] Batch writer to Snowflake (micro-batching, every 5 minutes)
- [ ] Raw event storage in Snowflake (immutable, append-only)
- [ ] Monitoring for consumer lag
- [ ] Dead letter queue for failed events

**Expected Output:**

```
Kafka (raw.orders.v1) â†’ Consumer â†’ Snowflake (BRONZE.orders_raw)
Kafka (raw.clicks.v1) â†’ Consumer â†’ Snowflake (BRONZE.clicks_raw)
```

---

### Stage 4: Flink Real-Time Processing (TODO)

**Goal:** Real-time attribution logic and stream enrichment

**To Implement:**

- [ ] Apache Flink job for click-to-order attribution
- [ ] Join clicks with orders in 30-minute window
- [ ] Calculate conversion metrics (CTR, conversion rate)
- [ ] Emit attributed events to new topic: `attributed.events.v1`
- [ ] Session management and user journey tracking
- [ ] Real-time alerting for anomalies

**Expected Output:**

```
Kafka (raw.clicks.v1 + raw.orders.v1) â†’ Flink â†’ attributed.events.v1
```

---

### Stage 5: Batch ETL with Spark + Airflow (TODO)

**Goal:** Scheduled data transformations and aggregations

**To Implement:**

- [ ] Apache Airflow for orchestration
- [ ] Spark jobs for data transformations
- [ ] SILVER layer: cleaned and validated data
- [ ] GOLD layer: aggregated metrics and KPIs
- [ ] Daily/hourly batch jobs
- [ ] Data quality checks

**Expected Tables:**

```
SILVER:
- orders_cleaned (validated, deduplicated)
- clicks_cleaned
- user_sessions (sessionized)

GOLD:
- daily_metrics (GMV, orders, users)
- food_item_performance
- user_cohorts
```

---

### Stage 6: Redis Billing + Analytics Dashboard (TODO)

**Goal:** Real-time billing and business intelligence

**To Implement:**

- [ ] Redis cache for real-time billing counters
- [ ] API for billing queries
- [ ] Metabase/Superset dashboard
- [ ] Key metrics visualization
- [ ] Alerts and notifications

---

## ðŸ“Š Current Status Summary

**Completed:** 2/6 stages (33%)

**Working Systems:**

- âœ… Kafka cluster (3 brokers, 2 topics, 9 partitions)
- âœ… Events Gateway with server-side UUID generation
- âœ… PostgreSQL with 2 tables (foods, orders)
- âœ… Food Catalog Service API
- âœ… Next.js web app with 3 pages
- âœ… Real-time event monitoring
- âœ… All code pushed to GitHub (9 commits)

**Next Milestone:** Snowflake Bronze Layer - consume Kafka events into data warehouse

---

## ðŸŽ¯ Architecture Principles Followed

1. âœ… **Self-contained events** - Events include all business context
2. âœ… **Database-first** - Orders stored in DB before Kafka
3. âœ… **Server-side ID generation** - UUID v4 for uniqueness at scale
4. âœ… **Separation of concerns** - Reference data (PostgreSQL) vs Events (Kafka)
5. âœ… **Observability** - Monitoring, logging, health checks
6. âœ… **Production-ready** - No collision risk, proper error handling
